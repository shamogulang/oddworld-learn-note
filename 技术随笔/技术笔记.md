<center><h1>技术笔记</h1></center>

### 1、nacos

windows 启动： startup.cmd -m standalone  nacos的单机启动



动态更新对应的源码入口地址：NacosContextRefresher



绑定数据类： DataObjectBinder



context刷新类： ContextRefresher 里面的方法refreshEnvironment会对差异就行处理



注意如果需要nacos的这个功能，需要引入jar包

```xml
        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
        </dependency>
```



动态更新的配置：

```yml
spring:
    cloud:
        nacos:
            config:
                shared-configs[0]:
                    data-id: error-code.yaml
                refreshable-dataids: error-code.yaml
```

也可以根据约定：nacos的动态更新是默认打开的：

所以这个配置不配置都没有问题：

```java
spring:
    cloud:
        nacos:
            config:
                auto-refresh: true # 是否启用动态刷新配置
                 
```



直接：运用名.properties, group 为默认group



注意这里会用配置上的最新配置，而不会使用${memoryTask.timeLimit.month:true}中的true





调试流程：

在经过了这个过程：org.springframework.boot.context.properties.bind.Binder#bind(org.springframework.boot.context.properties.source.ConfigurationPropertyName, org.springframework.boot.context.properties.bind.Bindable<T>, org.springframework.boot.context.properties.bind.BindHandler)后，properties的数据已经更新了



有bug，属性绑定的过程，有bug。

```java
@Override
	protected Map<Object, Object> merge(Supplier<Map<Object, Object>> existing, Map<Object, Object> additional) {
		Map<Object, Object> existingMap = getExistingIfPossible(existing);
		if (existingMap == null) {
			return additional;
		}
		try {
			existingMap.putAll(additional);
			return copyIfPossible(existingMap);
		}
		catch (UnsupportedOperationException ex) {
			Map<Object, Object> result = createNewMap(additional.getClass(), existingMap);
			result.putAll(additional);
			return result;
		}
	}
```

对于删除的过程，additional 这个属性的map是已经删除的了数据，或者是已经修改或者是已经增加的数据，但是里面有个existingMap，这个是旧的数据。所以当existingMap.putAll(additional)执行的时候，那么就会覆盖掉修改的，增加是增加的属性，那么对于删除的属性，还是使用旧的数据，所以就无法删除。

 getExistingIfPossible(existing) 这个方法，是通过反射去调用对应的配置的属性类的某个属性的get方法：

比如这里就是调用：

```java
@Component
@ConfigurationProperties(prefix = "error.code")
public class ErrorCodeCommon {
    private Map<String,String> systemCodeMap;
    
     public Map<String, ErrorCode> getSystemCodeMap() {
        return systemCodeMap;
    }
    
     public void setSystemCodeMap(Map<String, ErrorCode> systemCodeMap) {
        this.systemCodeMap = systemCodeMap;
    }
}
这个类的get的方法。当getXXX完，同时将上述的additional Map，加入后，再通过反射调用：setXXX方法，将属性设置为更新后的值。其实就是将上述的propertis实例的属性修改了，但是这个类的实例还是一开始ioc容器中的实例。那么这个过程就实现了动态更新。
```



解决方案：

1、可以通过将需要删除的属性，修改成空字符串，那么，对于错误码而言，当第三方服务的code获取到是“”,那么就等价于删除了这个错误码。

2、直接忽略，这种数据会有滞后性，直到下一次服务启动的时候，才会删除掉已经删除的数据。

3、省略对应的配置的类的属性getxxx方法，那么每次都会用最新的配置，不会考虑原来的配置。

目前采用第三种方案。



#### 1.2、nacos的bug2

```
2021-06-22 20:14:03.409|main|ERROR|c.a.n.c.c.impl.ClientWorker|IGNORE||||||||||||||[fixed-10.17.96.107_8847/] [sub-server] get server config exception, dataId=memory-task-limit.yaml, group=DEFAULT_GROUP, tenant=
java.net.ConnectException: [NACOS HTTP-GET] The maximum number of tolerable server reconnection errors has been reached
	at com.alibaba.nacos.client.config.http.ServerHttpAgent.httpGet(ServerHttpAgent.java:115)
	at com.alibaba.nacos.client.config.http.MetricsHttpAgent.httpGet(MetricsHttpAgent.java:48)
```



这里是集群情况（10.17.96.107:8847，10.17.96.107:8849，10.17.96.107:8851），如果为：

```
spring:
    cloud:
        nacos:
            discovery:
                server-addr: ${NACOS_SERVER:10.17.96.107:8847}/ # 你的nacos地址
            config:
                server-addr: ${NACOS_SERVER:10.17.96.107:8847} # 你的nacos地址
                
注意这里如果在config的配置后面加上一个/的话，会报错，就是上述的一个错，如下述的配置会报错

spring:
    cloud:
        nacos:
            discovery:
                server-addr: ${NACOS_SERVER:10.17.96.107:8847}/ # 你的nacos地址
            config:
                server-addr: ${NACOS_SERVER:10.17.96.107:8847}/ # 你的nacos地址
```



单机的情况下：







### 2、jackson

通讯录接口接入的时候，有些字段命名不规范，比如BP，两个都是大小，对于jackson来说如果是通过：toJson的方法，会将其转成小写。

```java
package com.zyhl.mcloud.orchestration.familycloud;

import lombok.Data;
import lombok.NoArgsConstructor;

/**
 * Copyright 2021 ChinaMobile Info. Tech Ltd. All rights reserved.
 *
 * @Author: jeffchan
 * @DATE: 2021/5/19 14:13
 **/
@Data
@NoArgsConstructor
public class MyTest {

    private String BP;
}

package com.zyhl.mcloud.orchestration.familycloud;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.zyhl.cloud.commons.utils.JsonUtil;

/**
 * Copyright 2021 ChinaMobile Info. Tech Ltd. All rights reserved.
 *
 * @Author: jeffchan
 * @DATE: 2021/5/19 14:13
 **/
public class Test {

    private static ObjectMapper mapper = new ObjectMapper();
    public static void main(String[] args) throws Exception {
        MyTest myTest = new MyTest();
        myTest.setBP("cjh");
        String s = mapper.writeValueAsString(myTest);
        System.out.println(s);
    }
}

 返回结果：发现结果被转成小写的了。
 {"bp":"cjh"}   
 如果需要还是保留原来的字段，需要在属性上面加上注解指定字段：@JsonProperty("BP")
    
 可以将一个对象转成map:
 mapper.readValue(json, Map.class); 注意转成的是一个LinkHashMap
```



### 3、yaml

将对象转成yaml格式的字符串：

String errorYaml = new YAMLMapper().writeValueAsString(jsonNodeTree);  

注意要引入依赖：

```xml
 <dependency>
       <groupId>com.fasterxml.jackson.dataformat</groupId>
       <artifactId>jackson-dataformat-yaml</artifactId>
       <version>2.10.2</version>
</dependency>
```



将json转成yaml的格式，可以使用下述方式：

```yaml
JsonNode jsonNodeTree = new ObjectMapper().readTree(JsonUtil.toJson(errorCodeYaml));
String errorYaml = new YAMLMapper().writeValueAsString(jsonNodeTree);         
```



### 4、mapstruct的bug记录

对于目标类：

```java
package com.zyhl.mcloud.orchestration.personalcloud.test;

import lombok.Data;

/**
 * Copyright 2021 ChinaMobile Info. Tech Ltd. All rights reserved.
 *
 * @Author: jeffchan
 * @DATE: 2021/5/27 18:35
 **/
@Data
public class Test {
    
    private String helloId;
    private String helloID;
}
```

还有源类：

```java
package com.zyhl.mcloud.orchestration.personalcloud.test;

import lombok.Data;

/**
 * Copyright 2021 ChinaMobile Info. Tech Ltd. All rights reserved.
 *
 * @Author: jeffchan
 * @DATE: 2021/5/27 18:36
 **/
@Data
public class TestTarget {

    private String helloID;
}

```



动作配置类：

```java
@Mapper(componentModel = "spring")
public interface TestConverter {

     Test toTest(TestTarget testTarget);
}
```

此时编译发现，映射不成功，将对应的Test中的两个属性的位置顺序改一下，就可以解决这个问题。



### 5、mysql

```java
Select CONCAT( 'drop table ', table_name, ';' ) 
FROM information_schema.tables 
Where table_name LIKE 'tabel_name_2019%';
```

```
// 补充三位数
String tableSuffix = StringUtils.leftPad(value, 3, "0");
```

给mysql查询的记录添加序号：

```
set @rownum=0;
select @rownum:=@rownum+1, t.* from memory_content_035 t where `owner` = "+8613501525064" and shooting_time is not null
```



创建存储过程：

```mysql
DELIMITER $$
USE `orche_memory_album_db`$$
DROP PROCEDURE IF EXISTS `pro_TableCreate`$$
 
CREATE DEFINER=`root`@`%` PROCEDURE `pro_TableCreate`(table_prefix VARCHAR(100), table_size INT, table_sql VARCHAR(2048))
BEGIN
DECLARE i INT;
DECLARE table_name VARCHAR(20);
SET i = 0;
SET table_prefix = CONCAT(table_prefix, '_');
WHILE i<table_size DO
IF i<10 THEN
SET table_name = CONCAT(table_prefix, '00', i);
ELSEIF i < 100 THEN
SET table_name = CONCAT(table_prefix,'0', i);
ELSE 
SET table_name = CONCAT(table_prefix, i);
END IF;
 
SET @csql = CONCAT('CREATE TABLE ',table_name, table_sql);
 
PREPARE create_stmt FROM @csql;
EXECUTE create_stmt;
SET i = i+1;
END WHILE;
END$$
DELIMITER ;
```

调用存储过程：

```mysql
CALL pro_TableCreate("memory_album", 10, "(
  `album_id` bigint(11) NOT NULL AUTO_INCREMENT,
  `album_name` varchar(32) NOT NULL COMMENT '相册名称',
  `module_type` tinyint(2) NOT NULL COMMENT '模块类型\n1.家庭云\n2.群组或共享群(预留)\n3.个人云',
  `service_id` varchar(32) DEFAULT NULL COMMENT '业务标识，取值取决于moduleType\n模块类型为1.家庭云时为cloudID\n模块类型为2.共享群时为groupID\n模块类型为3.个人云时无意义',
  `owner` varchar(32) NOT NULL COMMENT '所属人',
  `rule_id` int(11) NOT NULL COMMENT '规则编号',
  `begin_time` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '照片集开始时间',
  `end_time` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '照片集结束时间',
  `label` tinyint(2) DEFAULT '1' COMMENT '标签类型：1-时间，2-地点，3-事物',
  `off_time` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '下架时间',
  `status` tinyint(2) NOT NULL DEFAULT '1' COMMENT '状态：0.下架，1.上架',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL COMMENT '更新时间',
  `ext_0` varchar(1024) DEFAULT NULL COMMENT '扩展字段0',
  `ext_1` varchar(1024) DEFAULT NULL COMMENT '扩展字段1',
  PRIMARY KEY (`album_id`),
  KEY `idx_owner` (`owner`) USING HASH COMMENT '所属用户增加索引',
  KEY `idx_beginTime` (`begin_time`) USING BTREE,
  KEY `idx_endTime` (`end_time`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='回忆相册表'");
```



#### 分配权限：

```
Create USER 'test123'@'%'  IDENTIFIED By "test";  #创建用户

GRANT All On orches_memory_album_tiyan_db.memory_album_000 TO 'test123'@'%';  分配所有权限

GRANT SELECT on orches_memory_album_tiyan_db.memory_user  TO 'test123'@'%'; 分配查看权限
```







### 6、docker配置

```xml
springboot和docker的配置：
    <properties>
       <docker.plugin.version>1.0.0</docker.plugin.version>
       <docker.registry>10.164.9.5:9080/mcloud-orchestration-test</docker.registry>

    </properties>

<plugin>
                <groupId>com.spotify</groupId>
                <artifactId>docker-maven-plugin</artifactId>
                <version>${docker.plugin.version}</version>
                <executions>
                    <execution>
                        <id>build-image</id>
                        <phase>package</phase>
                        <goals>
                            <goal>build</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <imageName>${docker.registry}/${project.artifactId}</imageName>
                    <dockerDirectory>${project.basedir}/src/main/docker</dockerDirectory>
                    <forceTags>true</forceTags>
                    <imageTags>
                        <imageTag>${project.version}</imageTag>
                    </imageTags>
                    <resources>
                        <resource>
                            <targetPath>/</targetPath>
                            <directory>${project.build.directory}</directory>
                            <include>${project.build.finalName}.jar</include>
                        </resource>
                    </resources>
                </configuration>
 </plugin>
```



docker:没有开启服务：

docker login 10.164.9.5:9080
Username: admin
Password: 



wsl出问题：

```
启动 WSL 2时警告“参考的对象类型不支持尝试的操作”

管理员身份执行： netsh winsock reset  
```







#### 打包

1、首先是在父目录中package







### 7、soul

修改了pluginList的springCloud之后，该类的方法会动态更新SELECTOR_MAP的值：

```java
org.dromara.soul.plugin.base.cache.BaseDataCache#cacheSelectData
```

如果仅仅是增加这个是没用的，会报错（源码里发现没有日志信息）

需要到pluginList的logging中配置一个



这两个之后也是没有用，需要到 BaseConfig 的plugin中将springCloud的状态改成启用状态。



这样子之后还不行，需要到 BaseConfig中的metadata中增加一个映射，这个是用来匹配uri的



### 8、rocketmq

#### 8.1、docker集群

```java
docker run -d -p 9876:9876 -v /root/rocketmq/logs/data/namesrv/logs:/root/logs -v /root/rocketmq/logs/data/namesrv/store:/root/store --name rmqnamesrv -e "MAX_POSSIBLE_HEAP=100000000" rocketmqinc/rocketmq sh mqnamesrv
```

```java
配置文件： broker.conf

brokerClusterName = DefaultCluster
brokerName = broker-a
brokerId = 0
deleteWhen = 04
fileReservedTime = 48
brokerRole = ASYNC_MASTER
flushDiskType = ASYNC_FLUSH
brokerIP1 = 8.134.92.138
```



```
docker run -d -p 10911:10911 -p 10909:10909 -v  /root/rocketmq/data/broker/logs:/root/logs -v  /root/rocketmq/rocketmq/data/broker/store:/root/store -v  /root/rocketmq/conf/broker.conf:/opt/rocketmq/conf/broker.conf --name rmqbroker --link rmqnamesrv:namesrv -e "NAMESRV_ADDR=namesrv:9876" -e "MAX_POSSIBLE_HEAP=200000000" rocketmqinc/rocketmq sh mqbroker -c /opt/rocketmq/conf/broker.conf
```



控制端：

```
docker pull styletang/rocketmq-console-ng　
```

启动

```
docker run -e "JAVA_OPTS=-Drocketmq.config.namesrvAddr=8.134.92.138:9876 -Drocketmq.config.isVIPChannel=false" -p 8080:8080 -t styletang/rocketmq-console-ng
```



#### 8.2、windows搭建

配置两个环境变量

```
ROCKETMQ_HOME="D:\rocketmq"
NAMESRV_ADDR="localhost:9876"
```

进入到启动目录：

```
.\bin\mqnamesrv.cmd

.\bin\mqbroker.cmd -n localhost:9876 autoCreateTopicEnable=true
```

安装对应的console的控制台：

```
下载项目：https://github.com/apache/rocketmq-externals
然后在idea中跑起来就可以了
```









### 9、搭建redis集群

#### 9.1、内网集群

1、首先需要给redis搭建专用网络

docker  network create redis --subnet  172.38.0.0/16

2、通过脚本给redis生成对应的配置文件

```
for port in $(seq 1 6);\
do \
mkdir -p /mydata/redis/node-${port}/conf
touch /mydata/redis/node-${port}/conf/redis.conf
cat <<EOF>>/mydata/redis/node-${port}/conf/redis.conf
port 6379
bind 0.0.0.0
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
cluster-announce-ip 172.38.0.1${port}
cluster-announce-port 6379
cluster-announce-bus-port 16379
appendonly yes
EOF
done
```

启动六个redis

```
docker run -p 6371:6379 -p 16371:16379 --name redis-1 -v /mydata/redis/node-1/data:/data -v /mydata/redis/node-1/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.11 redis redis-server /etc/redis/redis.conf

docker run -p 6372:6379 -p 16372:16379 --name redis-2 -v /mydata/redis/node-2/data:/data -v /mydata/redis/node-2/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.12 redis redis-server /etc/redis/redis.conf

docker run -p 6373:6379 -p 16373:16379 --name redis-3 -v /mydata/redis/node-3/data:/data -v /mydata/redis/node-3/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.13 redis redis-server /etc/redis/redis.conf

docker run -p 6374:6379 -p 16374:16379 --name redis-4 -v /mydata/redis/node-4/data:/data -v /mydata/redis/node-4/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.14 redis redis-server /etc/redis/redis.conf

docker run -p 6375:6379 -p 16375:16379 --name redis-5 -v /mydata/redis/node-5/data:/data -v /mydata/redis/node-5/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.15 redis redis-server /etc/redis/redis.conf

docker run -p 6376:6379 -p 16376:16379 --name redis-6 -v /mydata/redis/node-6/data:/data -v /mydata/redis/node-6/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.16 redis redis-server /etc/redis/redis.conf
```



进入其中一个redis:

docker exec -it redis-1 /bin/sh

创建集群信息：

```java
redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1
```



#### 9.2、外网集群

定制的集群模板：

```
##设置外部网络连接redis服务，默认是yes，即开启。开启protected-mode保护模式，需配置bind ip或者设置访问密码，关闭protected-mode模式，此时外部网络可以直接访问
protected-mode no
##节点端口
port ${PORT}
##指定redis是否要用守护线程的方式启动，默认no
daemonize no   (对于docker这个要打开，不然的话，docker的redis启动了一下就会关闭)
##持久化模式
appendonly yes
##cluster集群模式
cluster-enabled yes
##集群配置名
cluster-config-file nodes.conf
##超时时间
cluster-node-timeout 15000
##实际为各节点网卡分配ip
cluster-announce-ip 外网ip  这里要配置你的外网的ip
##节点映射端口
cluster-announce-port ${PORT}
##节点总线端
cluster-announce-bus-port 1${PORT}
##连接密码（可选）
requirepass 123456
```

修改模板的脚本：

```
for port in `seq 7001 7006`; do \
  base=6999 \
  && ip=$[port-base] \
  && mkdir -p ${port}/conf \
  && PORT=${port} TEMP=${ip} envsubst < redis-cluster2.tmpl > ${port}/conf/redis.conf \
  && mkdir -p ${port}/data;\
done
```



然后创建一个网络：

```
docker network create redis-net
```



然后执行下面脚本：

```
for port in `seq 7001 7006`; do
  docker run -p ${port}:${port} -p 1${port}:1${port} --name redis-${port} \
  -v {这里是你的生成的目录}/${port}/conf/redis.conf:/usr/local/etc/redis/redis.conf \
  -v {这里是你的生成的目录}/${port}/data:/data --name redis-${port} --net redis-net -d  redis  redis-server /usr/local/etc/redis/redis.conf;
done
```



设置集群信息：

```
redis-cli  --cluster create 8.134.92.138:7001 8.134.92.138:7002 8.134.92.138:7003 8.134.92.138:7004 8.134.92.138:7005 8.134.92.138:7006 --cluster-replicas 1 -a 123456
```



进入其中一个redis:

```
docker exec -it redis-7001 /bin/bash
```

auth 123456

查看集群信息：

```
8.134.92.138:7001> cluster nodes
d6d0ef7fdb97e5d69f9282b9d14237858f1a71df 8.134.92.138:7005@17005 slave 12ec7f95374a46dddd524f26a64ad60ff4a17098 0 1623748403000 1 connected
12ec7f95374a46dddd524f26a64ad60ff4a17098 8.134.92.138:7001@17001 myself,master - 0 1623748403000 1 connected 0-5460
b44c602497249083ae21a1eba7689ecccd1c4c47 8.134.92.138:7006@17006 slave 2c0b1a4678c1925a8b05ffd9daa379dbc1f1475f 0 1623748403000 2 connected
ae83a1a76844b244b0379e24cf8c15509709c1a2 8.134.92.138:7003@17003 master - 0 1623748403512 3 connected 10923-16383
848b471814b2b3738593029bb390303ad6480f8f 8.134.92.138:7004@17004 slave ae83a1a76844b244b0379e24cf8c15509709c1a2 0 1623748404515 3 connected
2c0b1a4678c1925a8b05ffd9daa379dbc1f1475f 8.134.92.138:7002@17002 master - 0 1623748402000 2 connected 5461-10922
```



#### 9.3、redission的删除

使用redisssion如果是用jakson的方式序列化，那么对应的key，会加多一个双引号。

通过命令行删除的时候，需要：

```
10.164.9.4:30013> hgetall memory:rule:festival:CHINESE_NEW_YEAR:2021:120
1) "\"+8619802021875\""
2) "true"

那么删除对应的subkey的方式是：
hdel memory:rule:festival:CHINESE_NEW_YEAR:2021:120  "\"+8619802021875\""
```



### 10、github的两个镜像地址

```
https://github.com.cnpmjs.org
https://hub.fastgit.org
```



### 11、k8s集群

博客地址：

https://www.cnblogs.com/gcxblogs/p/13710023.html

```
docker tag a6ebd1c1ad98 k8s.gcr.io/kube-proxy:v1.21.2
docker tag f917b8c8f55b k8s.gcr.io/kube-scheduler:v1.21.2
docker tag 106ff58d4308 k8s.gcr.io/kube-apiserver:v1.21.2
docker tag ae24db9aa2cc k8s.gcr.io/kube-controller-manager:v1.21.2
docker tag 0369cf4303ff k8s.gcr.io/etcd:3.4.13-0
docker tag 0f8457a4c2ec k8s.gcr.io/pause:3.4.1
docker tag 296a6d5035e2 k8s.gcr.io/coredns/coredns:v1.8.0


docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.21.2
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.21.2
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.21.2
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.21.2
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.4.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0
docker pull /coredns/coredns:1.8.0
```



执行的配置文件：

```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.29.98.73
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-01
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: 1.21.2
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
kubeProxy:
  config:
    featureGates: SupportIPVSProxyMode=true  #不同版本这里的配置不一样
mode: ipvs

```



kubectl get node 查看节点的启动情况：



```
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```



添加环境变量：

```
export KUBECONFIG=/etc/kubernetes/admin.conf
```

问题：

```
F0622 15:25:07.782646       1 server.go:482] failed complete: unrecognized feature gate: SupportIPVSProxyMode

k8s"failed to set bridge addr: "cni0" already has an IP address different from 10.244.1.1/24"

这个是对应的pod所在的主机的flannel配置不一致导致的：
cat /run/flannel/subnet.env  可以发现对应的配置（注意：分配到该主机的pod用的ip是这个文件下的FLANNEL_SUBNET配置）

FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true

如果这个跟：cni0的不一致的话，会报上述的那个错误
4: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether e2:f9:a9:ea:a3:24 brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.1/24 brd 10.244.0.255 scope global cni0
       valid_lft forever preferred_lft forever
```





```
创建一个namespace

kubectl create ns  dev 

查看namespace
kubectl get ns  [name]

kubectl get ns -o yaml   通过yaml的格式展示命名空间



查看所有的pod
kubectl get pods --all-namespaces

跑某个镜像：
kubectl run nginx-offi --image=nginx

查看详细的信息
kubectl get pod -o wide  
```



kubectl describe ns dev

```
Name:         dev
Labels:       kubernetes.io/metadata.name=dev
Annotations:  <none>
Status:       Active

No resource quota.

No LimitRange resource.
```



删除命名空间

kubectl delete ns  dev



通过文件的形式进行创建：dev.yaml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
```

然后通过： kubectl apply -f  dev.yaml  创建和更新

删除： kubectl  delete  -f   dev.yaml



#### 11.1、pod的使用

kubectl get pod -n  kube-system   获取对应命名空间中的pod

启用一个运用：

kubectl run nginx --image=nginx:1.17.1 --port=80 --namespace dev



### 12、阿里云

##### 域名管理

入口： https://dc.console.aliyun.com/next/index?spm=a2c81.a01b12c.products-recent.ddomain.13231127TjlppF#/domain/list/all-domain





### 13、frp内网穿透

frp的下载地址： centos的云服务器

```
https://github.com/fatedier/frp/releases/download/v0.38.0/frp_0.38.0_linux_amd64.tar.gz
```

解压：



server: frps

```
修改 frps.ini 文件，设置 http 访问端口为 8080

bind_port = 7000 #服务启动占用端口
vhost_http_port = 8080 # 转发端口
```



下载windows的客户端：

```
https://github.com/fatedier/frp/releases/download/v0.38.0/frp_0.38.0_windows_amd64.zip
```





client:  frpc

```
[common]
server_addr = 45.156.23.186
server_port = 7000


[web] 
#http访问的穿透，客户端可以搭建一个nginx，这样子可以访问多个服务，不这样子的话，就需要有多个域名，如果只有一个域名，就可以加nginx来处理。
#加nginx的情况，可以根据请求的路径来转发，但是端口号是一样的。
type = http
local_ip =127.0.0.1
local_port = 80
custom_domains = www.caraliu.top

[mysql] 
#数据库的穿透，不需要用到nginx,就可以转发多个。
#可以穿透多个，访问时：host=域名，端口号：33333，密码和账号一样。
type = tcp
local_ip = 10.164.9.5
local_port = 31370
remote_port = 33333


[mysql1] 
#可以穿透多个，访问时：host=域名，端口号：33332，密码和账号一样。
type = tcp
local_ip = 127.0.0.1
local_port = 3306
remote_port = 33332
```



```
user  nginx;
worker_processes  auto;

error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;

    server {

       listen 80;
       server_name  192.168.142.17;


       location ^~  /xxl-job-admin/ {
          add_header backendIP $upstream_addr;
          add_header backendIP $upstream_status;
          proxy_pass  http://10.164.9.5:30800;
       }

    }

    server {

       listen 80; 
       server_name *.caraliu.top;  #注意这里要用一开始的域名来匹配，不然匹配不了，会一直404，本来我以为是127.0.0.1，其实不是

       location   /xxl-job-admin/ {
         add_header backendIP $upstream_addr;
         add_header backendIP $upstream_status;
         proxy_pass http://10.164.9.5:30800;
         proxy_redirect off;
         proxy_set_header X-Real-IP $remote_addr;
         proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
         proxy_set_header X-Nginx_Porxy true;

       }

    }
}
```



### 14、rabbitmq





### 15、领域驱动设计

```
https://zhuanlan.zhihu.com/p/366395817   没有的话都看些这些文章吧，一共五篇 后续我们的重构思想将依托DDD的架构理念。
```



###  16、kafka的学习

#### 1、基本概念

```yaml
broker: kfaka集群包含一个或者多个服务器，这种服务器被称为broker
topic: 每条发布到kafka的集群消息都有一个类别，这个类别称为topic。（物理上不同topic的消息分开存储，逻辑上一个topic的消息虽然存在一个或者多个broker上但是用户只需指定消息的topic即可生产或者消费数据，而不必关系数据存在何处）
partition: partition是物理上的概念，每个topic包含一个或者多个partion
producer: 负责发送消息到kafka broker
consumer: 消息消费者，想kafka broker读取消息的客户端。
consumer group: 每个消费者都属于一个特定的消费者组（不指定默认为group）
topic&partition: topic在逻辑上可以理解为一个队列，每条消费都必须指定他的topic,可以简单理解为必须指明吧这条消息放进到哪个queue里。未来使得kafka的吞吐率可以线性提高，物理上吧topic分为一个或者多个partition。每个partition在物理上对应一个文件夹，该文件夹下存储这个partition的所有消息和索引文件。
一个topic为一类消息，每条消息必须指定一个topic,物理上，一个topic分为一个或者多个partition,每个partition有多个副本分布在不同的broker中。
```



### 17、新的服务部署方式

先将镜像push到目前的habour上



18、访问量和云服务器的关系：

```
你好，我想拿主平台，个人云分类接口（视频，文档，音频）的访问量。

并发数计算 PV 的粗算计算公式是
qps（或并发数） x 86400（秒）÷ 2 （分昼夜）
所以 PV 100万 粗算来并发数只有 23 。

一台 4 核 4G 内存的机器可以抗住 100左右的并发数。

目前的情况：
   


TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。TPS包括一条消息入和一条消息出，加上一次用户数据库访问。（业务TPS = CAPS × 每个呼叫平均TPS）

QPS：每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。
```





18、es知识点：

对key倒叙排

```
{"size":0,

    "aggs":{
        "xxx":{
            "date_histogram":{
                "field":"updTm",
                "interval":"day",
                "min_doc_count": 1,
                "format":"yyyy-MM-dd",
                "order": {
                  "_key": "desc"
                }
            }
        }
    }
}
```

聚合后分页

```
{"size":0,

    "aggs":{
        "sales":{
            "date_histogram":{
                "field":"updTm",
                "interval":"month",
                "format":"yyyy-MM"
            },
            "aggs":{
                "bucket_truncate":{
                    "bucket_sort":{
                        "from":1,
                        "size":1
                    }
                }
            }
        }
    }
}
```



### 18、hbase技术

```
docker pull harisekhon/hbase

docker run --name hbase0 -p 16010:16010  harisekhon/hbase


进入到bash:
docker exec -it e4367c8f1ecf  bash
 
连接hbase:
hbase shell

创建表和对用的列名
create 'table' , 'col1','col2'
添加记录
put 'table', 'row1', 'col1', 'value1'

put 'table', 'row3', 'col1:age', 11  # 添加了一个子列age

查看记录：
 get 'table' , 'row1'
 
COLUMN                          CELL
 col1:                          timestamp=1641536445142, value=value1
 
 删除记录：（可以单个列删除）
 delete 'table' , 'row1', 'col1', 'col2'
 
 删除一张表：
 disable 'table'  禁止表
 drop 'table'   删除表
 
 查看表：
 scan 'table'
 查看表的某一列：
 scan 'table', {COLUMN=>'col2'}
 
 查看表的描述：
  describe 'table'
  
  添加一个列：
  alter 'table', 'col3'
  删除一个列：
  alter 'table', {NAME => 'col3', METHOD => 'delete'}
  删除整行：
  deleteall 'table', 'row1'
```



